{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset,RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "eps = 1\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batchflag = True\n",
    "batchsize = 128\n",
    "\n",
    "start = 0.\n",
    "end = 1.\n",
    "x = np.linspace(start,end,100 )\n",
    "y = np.linspace(start,end,100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "x = np.reshape(x, (np.size(x[:]),1))\n",
    "y = np.reshape(y, (np.size(y[:]),1))\n",
    "\n",
    "\n",
    "\n",
    "def actual_soln(eps):\n",
    "    return (x**2 + eps**2 * (1 - np.exp(-x/eps))**2 )*((x-1)**2)*(y**2)*((y-1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(soln,soln_name):\n",
    "    x = np.linspace(start,end,100);y = np.linspace(start,end,100)\n",
    "    x,y = np.meshgrid(x,y)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(x,y,soln.reshape(100,100))\n",
    "    plt.title(soln_name)\n",
    "    plt.show()\n",
    "\n",
    "class Swish(nn.Module):\n",
    "\tdef __init__(self, inplace=True):\n",
    "\t\tsuper(Swish, self).__init__()\n",
    "\t\tself.inplace = inplace\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif self.inplace:\n",
    "\t\t\tx.mul_(torch.sigmoid(x))\n",
    "\t\t\treturn x\n",
    "\t\telse:\n",
    "\t\t\treturn x * torch.sigmoid(x)\n",
    "\t\n",
    "\n",
    "class FBPINN(nn.Module):\n",
    "\thid_dim = 128\n",
    "\tinput_dim = 2 \n",
    "\tdef __init__(self):\n",
    "\t\tsuper(FBPINN, self).__init__()\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.lin0 = nn.Linear(self.input_dim,self.hid_dim)\n",
    "\t\tself.lin = nn.Linear(self.hid_dim,self.hid_dim)\n",
    "\t\tself.lin1 = nn.Linear(self.hid_dim,1)\n",
    "\t\tself.swish = Swish()\n",
    "\tdef forward(self,x):\t\t\n",
    "\t\ttanh1 = self.tanh(x)\n",
    "\t\ttanh2 = self.tanh(1 - x)\n",
    "\t\ttanh11 = (tanh1[:,0].unsqueeze(1))*(tanh1[:,1].unsqueeze(1))*(tanh1[:,0].unsqueeze(1))*(tanh1[:,1].unsqueeze(1))\n",
    "\t\ttanh22 = (tanh2[:,0].unsqueeze(1))*(tanh2[:,1].unsqueeze(1))*(tanh2[:,0].unsqueeze(1))*(tanh2[:,1].unsqueeze(1))\n",
    "\t\tx = self.lin0(x)\n",
    "\t\tx = self.swish(x)\n",
    "\t\tx = self.lin(x)\n",
    "\t\tx = self.swish(x)\n",
    "\t\tx = self.lin(x)\n",
    "\t\tx = self.swish(x)\n",
    "\t\tx = self.lin(x)\n",
    "\t\tx = self.swish(x)\n",
    "\t\tx = self.lin1(x)\n",
    "\t\tout = x*tanh11*tanh22\n",
    "\t\treturn  out\n",
    "\t\n",
    "def lag_coeffs(num):\n",
    "\t\tlis = [np.array([1]),np.array([1,-1])]\n",
    "\t\tfor n in range(1,num-1):\n",
    "\t\t\txl1 = np.concatenate((np.array([0]),lis[n]),axis = 0)\n",
    "\t\t\tl1 = np.concatenate((lis[n],np.array([0])),axis = 0)\t\t\n",
    "\t\t\tl0 = np.concatenate((lis[n-1],np.array([0,0])),axis = 0)\n",
    "\t\t\tl = ((2*n + 1)*l1 - xl1 - n*l0)/(n + 1)\n",
    "\t\t\tlis.append(l)\n",
    "\t\tfor n in range(num):\n",
    "\t\t\tlis[n] = np.concatenate((lis[n],np.array([0]*(num-1-n))),axis = 0)\n",
    "\t\treturn lis\n",
    "\n",
    "def poly(coeffs,x):\n",
    "\tsum = torch.zeros_like(x)\n",
    "\tfor i,coeff in enumerate(coeffs):\n",
    "\t\tsum = sum + coeff*(x**i)\n",
    "\treturn sum\n",
    "\n",
    "class Lag_PINN(nn.Module):\n",
    "\tdef __init__(self,basis_num):\n",
    "\t\tsuper(Lag_PINN,self).__init__()\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.basis_num = basis_num\n",
    "\t\tself.main = nn.Sequential(\n",
    "\t\t\t#nn.Linear(2*basis_num,2*basis_num),\n",
    "\t\t\tnn.Linear(2*basis_num,1))\t\t\n",
    "\tdef forward(self,input):\n",
    "\t\ttanh1 = self.tanh(input)\n",
    "\t\ttanh2 = self.tanh(1 - input)\n",
    "\t\ttanh11 = (tanh1[:,0].unsqueeze(1))*(tanh1[:,1].unsqueeze(1))*(tanh1[:,0].unsqueeze(1))*(tanh1[:,1].unsqueeze(1))\n",
    "\t\ttanh22 = (tanh2[:,0].unsqueeze(1))*(tanh2[:,1].unsqueeze(1))*(tanh2[:,0].unsqueeze(1))*(tanh2[:,1].unsqueeze(1))\n",
    "\t\tcoeffs = lag_coeffs(self.basis_num)\n",
    "\t\tx = input[:,0]\n",
    "\t\tt = input[:,1]\n",
    "\t\tnetin = torch.Tensor([])\n",
    "\t\tfor i in range(self.basis_num):\n",
    "\t\t\tnetin = torch.cat((netin,poly(coeffs[i],x.view(-1,1))),1)\n",
    "\t\tfor i in range(self.basis_num):\n",
    "\t\t\tnetin = torch.cat((netin,poly(coeffs[i],t.view(-1,1))),1)\n",
    "\t\tnetout = self.main(netin)\n",
    "\t\treturn netout*tanh11*tanh22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(device,x,y,eps,learning_rate,epochs,batch_flag,batch_size):\n",
    "\t\n",
    "\txnet = torch.Tensor(x)\n",
    "\tynet = torch.Tensor(y) \n",
    "\t\n",
    "\tif(batch_flag):\n",
    "\t\tdataset = TensorDataset(xnet,ynet)\n",
    "\t\tdataloader = DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers = 0,drop_last = True )\n",
    "\t\tprint(len(dataloader))\n",
    "\t\t\n",
    "\tnet = FBPINN()# Lag_PINN(10)#FBPINN()#.to(device)\n",
    "\t\n",
    "\tdef init_normal(m):\n",
    "\t\tif type(m) == nn.Linear:\n",
    "\t\t\tnn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\tnet.apply(init_normal)\n",
    "\n",
    "\toptimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "\n",
    "\tdef Loss_criterion(xnet,ynet):\n",
    "\t\txnet.requires_grad = True\n",
    "\t\tynet.requires_grad = True\n",
    "\t\tpoints = torch.cat((xnet,ynet),1) \n",
    "\t\tU = net(points)\n",
    "\t\tU = U.view(len(U),-1)\n",
    "\t\t\n",
    "\t\tsoln = (xnet**2 + eps**2 * (1 - torch.exp(-xnet/eps))**2 )*((xnet-1)**2)*(ynet**2)*((ynet-1)**2)\n",
    "\n",
    "\t\tsoln_x = torch.autograd.grad(soln,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_xx = torch.autograd.grad(soln_x,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_xxx = torch.autograd.grad(soln_xx,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_xxxx = torch.autograd.grad(soln_xxx,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_y = torch.autograd.grad(soln,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_yy = torch.autograd.grad(soln_y,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_yyy = torch.autograd.grad(soln_yy,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_yyyy = torch.autograd.grad(soln_yyy,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_xxy = torch.autograd.grad(soln_xx,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tsoln_xxyy = torch.autograd.grad(soln_xxy,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\t\n",
    "\t\tf = (eps**2)*(soln_xxxx + soln_yyyy + 2*soln_xxyy) - (soln_xx + soln_yy)\n",
    "\t\t\n",
    "\t\tU_x = torch.autograd.grad(U,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_xx = torch.autograd.grad(U_x,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_xxx = torch.autograd.grad(U_xx,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_xxxx = torch.autograd.grad(U_xxx,xnet,grad_outputs=torch.ones_like(xnet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_y = torch.autograd.grad(U,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_yy = torch.autograd.grad(U_y,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_yyy = torch.autograd.grad(U_yy,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_yyyy = torch.autograd.grad(U_yyy,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_xxy = torch.autograd.grad(U_xx,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tU_xxyy = torch.autograd.grad(U_xxy,ynet,grad_outputs=torch.ones_like(ynet),create_graph = True,only_inputs=True)[0]\n",
    "\t\tloss1 = (eps**2)*(U_xxxx + U_yyyy + 2*U_xxyy) - (U_xx + U_yy) - f \n",
    "\t\t\n",
    "\t\treturn nn.MSELoss()(loss1,torch.zeros_like(loss1)) \n",
    "\n",
    "\tlosses = []\n",
    "\ttic = time.time()\n",
    "\n",
    "\tif(batch_flag):\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tif epoch == 30:\n",
    "\t\t\t\tlearning_rate = 0.00001\n",
    "\t\t\t\tnew_optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "\t\t\t\toptimizer = new_optimizer\n",
    "\t\t\tfor batch_idx, (x_in,y_in) in enumerate(dataloader):\n",
    "\n",
    "\t\t\t\tnet.zero_grad()\n",
    "\t\t\t\tloss = Loss_criterion(x_in,y_in)\n",
    "\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\t\toptimizer.step() \n",
    "\t\t\t\tif batch_idx % 20 ==0:\n",
    "\t\t\t\t\tprint('Train Epoch: {} \\tLoss: {:.20f}'.format(epoch, loss.item()))\n",
    "\n",
    "\t\t\tpoints = torch.cat((xnet,ynet),1)\n",
    "\t\t\tU = net(points)\n",
    "\t\t\tz = U.detach().numpy()\n",
    "\t\t\tactual_loss = np.square(actual_soln(eps) - z).mean()\n",
    "\t\t\tprint('\\nAfter Epoch {}, \\t Actual solution loss: {:.20f}\\n'.format(\n",
    "\t\t\t\tepoch, actual_loss))\n",
    "\t\t\tif epoch % 1 == 0:\n",
    "\t\t\t\tplot_graph(z,'Predicted solution')\n",
    "\t\t\t\n",
    "\t\t\tlosses.append([loss.item(),actual_loss])\n",
    "\n",
    "\telse:\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tif epoch == 20:\n",
    "\t\t\t\tlearning_rate = 0.00001\n",
    "\t\t\t\tnew_optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "\t\t\t\toptimizer = new_optimizer\n",
    "\t\t\n",
    "\t\t\tnet.zero_grad()\n",
    "\t\t\tloss = Loss_criterion(xnet,ynet)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t\n",
    "\t\t\toptimizer.step() \n",
    "\t\t\tpoints = torch.cat((xnet,ynet),1)\n",
    "\t\t\tU = net(points)\n",
    "\t\t\tz = U.detach().numpy()\n",
    "\t\t\tactual_loss = np.square(actual_soln(eps) - z).mean()\n",
    "\t\t\tprint('\\nAfter Epoch {}, \\t Actual solution loss: {:.20f}\\n'.format(\n",
    "\t\t\t\tepoch, actual_loss))\n",
    "\t\t\tif epoch % 5 == 0:\n",
    "\t\t\t\tplot_graph(z,'Predicted solution')\n",
    "\t\t\t\n",
    "\t\t\tlosses.append([loss.item(),actual_loss])\n",
    "\n",
    "\ttoc = time.time()\n",
    "\telapseTime = toc - tic\n",
    "\tprint (\"Time elapsed = \", elapseTime)\n",
    "\n",
    "\tnet_in = torch.cat((xnet,ynet),1)\n",
    "\toutput = net(net_in)  \n",
    "\t\n",
    "\treturn output,losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(actual_soln(eps),'Actual solution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bsaty\\workspace\\bilaplacian\\Ex1\\FBPINN.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bsaty/workspace/bilaplacian/Ex1/FBPINN.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output,losses \u001b[39m=\u001b[39m train(device,x,y,eps,learning_rate,epochs,batchflag,batchsize)\n",
      "\u001b[1;32mc:\\Users\\bsaty\\workspace\\bilaplacian\\Ex1\\FBPINN.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bsaty/workspace/bilaplacian/Ex1/FBPINN.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m net\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bsaty/workspace/bilaplacian/Ex1/FBPINN.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m loss \u001b[39m=\u001b[39m Loss_criterion(x_in,y_in)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bsaty/workspace/bilaplacian/Ex1/FBPINN.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bsaty/workspace/bilaplacian/Ex1/FBPINN.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bsaty/workspace/bilaplacian/Ex1/FBPINN.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m \u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bsaty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bsaty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output,losses = train(device,x,y,eps,learning_rate,epochs,batchflag,batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = output.detach().numpy()\n",
    "plot_graph(z,\"Predicted Solution\")\n",
    "plot_graph(actual_soln(eps),\"Actual Solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 1D tensor\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Unsqueeze to add a new dimension (convert to a 2D tensor)\n",
    "y = x.unsqueeze(0)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
